{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42f714fe",
   "metadata": {},
   "source": [
    "En este archivo, se explica e implementa el proceso de calcular las distancias a los vehículos que se encuentran en el mismo carril que nuestro coche.\n",
    "\n",
    "Empezamos importando las líbrerías y creando los directorios necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26f1de41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: c:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\display_elements/distance_prediction/images\n",
      "Videos: c:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\display_elements/distance_prediction/videos\n",
      "Resulting images: c:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\results/distance_prediction/images\n",
      "Resulting videos: c:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\results/distance_prediction/videos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F_torch\n",
    "import torch.nn.functional as F_nn\n",
    "\n",
    "from train import get_model_instance_segmentation\n",
    "from predictions import get_predictions, draw_predictions\n",
    "from road_lines_sobel import road_line_filter_image, calculate_road_lines, get_road_mask\n",
    "\n",
    "VEHICLE_CONFIG = {\n",
    "    1:  1.8,\n",
    "    2:  2.6,\n",
    "    3:  2.6,\n",
    "    4:  2.4,\n",
    "    5:  2.5,\n",
    "    6:  0.8,\n",
    "    7:  0.6\n",
    "}\n",
    "FOCAL_LENGTH = 1000\n",
    "\n",
    "CWD = os.getcwd()\n",
    "PATH_MODEL = os.path.join(CWD, \"final_model/maskrcnn_cityscapes.pth\")\n",
    "PATH_IMAGES = os.path.join(CWD, \"display_elements/distance_prediction/images\")\n",
    "PATH_VIDEOS = os.path.join(CWD, \"display_elements/distance_prediction/videos\")\n",
    "PATH_RESULT_IMAGES = os.path.join(CWD, \"results/distance_prediction/images\")\n",
    "PATH_RESULT_VIDEOS = os.path.join(CWD, \"results/distance_prediction/videos\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(PATH_RESULT_IMAGES)\n",
    "    os.mkdir(PATH_RESULT_VIDEOS)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "print(f\"Images: {PATH_IMAGES}\")\n",
    "print(f\"Videos: {PATH_VIDEOS}\")\n",
    "print(f\"Resulting images: {PATH_RESULT_IMAGES}\")\n",
    "print(f\"Resulting videos: {PATH_RESULT_VIDEOS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf552e",
   "metadata": {},
   "source": [
    "## Cargar el modelo\n",
    "\n",
    "Primero, debemos cargar el modelo entrenado con el fine-tunning, para realizar las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "198bc068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and ready to inference.\n"
     ]
    }
   ],
   "source": [
    "# Set the acceleration device (or the CPU if cuda is not available)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Get the model pre-trained model\n",
    "num_classes = 8\n",
    "model = get_model_instance_segmentation(8)\n",
    "\n",
    "# Load the calculated weights in fine-tunning process\n",
    "data_model = torch.load(PATH_MODEL, map_location=device, weights_only=False)\n",
    "model.load_state_dict(data_model['model_state_dict'])\n",
    "\n",
    "# Move the model to the acceleration device and set the model into evaluation mode\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "print(\"Loaded model and ready to inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac1090",
   "metadata": {},
   "source": [
    "## Obtener los vehículos en el mismo carril\n",
    "\n",
    "Lo primero que se debe hacer es filtrar aquellos vehículos que no se encuentran en el carril de nuestro vehículo, para quedarse con los vehículos objetivo.\n",
    "\n",
    "A continuación se implementa una función que comprueba si un vehículo se encuentra en el carril, comprobando si el centro inferior de la máscara del vehículo choca con la máscara del carril."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e67b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_lane(lane_mask, box):\n",
    "    \"\"\" Verify wether the vehicle is over the lane \"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    cx = int((x1 + x2) / 2)\n",
    "    cy = int(y2) - 10 # 10px arriba del límite inferior para asegurar contacto\n",
    "    \n",
    "    if 0 <= cx < lane_mask.shape[1] and 0 <= cy < lane_mask.shape[0]:\n",
    "        # Si el canal verde de la máscara tiene contenido\n",
    "        return lane_mask[cy, cx, 1] > 0 \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cbd89",
   "metadata": {},
   "source": [
    "## Representar distancia\n",
    "\n",
    "Para poder representar graficamente la distancia, se implementa una función que devuelve un gradiente de color asociado a una distancia: cuanto más cerca más rojo, y cuanto más lejos más verde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5c1c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_color(dist):\n",
    "    \"\"\" Generate RGB color gradient: Red (near) -> Green (far) \"\"\"\n",
    "    if dist < 10: return (0, 0, 255) # Rojo\n",
    "    if dist > 50: return (0, 255, 0) # Verde\n",
    "    \n",
    "    # Interpolación\n",
    "    ratio = (dist - 10) / 40\n",
    "    g = int(255 * ratio)\n",
    "    r = int(255 * (1 - ratio))\n",
    "    return (r, g, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06620f05",
   "metadata": {},
   "source": [
    "## Dibujar resultados en imagen\n",
    "\n",
    "A continuación, se define una función que, dada una imagen, dibuja sobre ella las máscaras de segmentación y las bounding boxes etiquetadas con la clase que el modelo a asignado a esa instancia. Y para los vehículos que se encuentran en el carril, calcula el gradiente de color en base a la distancia, y añade la distancia a la etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e9c937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_predictions_in_lanes(image_tensor, pred_masks, pred_boxes, pred_labels, lane_mask):\n",
    "    \"\"\"\n",
    "        This method draw over a given image the segmentation masks and bounding boxes of given predictions, \n",
    "        and for those vehicles on the lane mask, calculate and represents their distance\n",
    "    Args:\n",
    "        image_tensor: the image in Tensor format\n",
    "        pred_masks: The segmentation masks\n",
    "        pred_boxes: The bounding boxes\n",
    "        pred_labels: The labels for each predicted class\n",
    "        lane_mask: The mask for the own lane\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        processed_image: the image with masks and bounding boxes drawn on it\n",
    "    \"\"\"\n",
    "    \n",
    "    # if there are no elements detected, returns the original image\n",
    "    if len(pred_boxes) == 0:\n",
    "        return (image_tensor * 255).to(torch.uint8).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Lists with colors and labels for each detected vehicle\n",
    "    dynamic_colors = []\n",
    "    dynamic_labels = []\n",
    "    \n",
    "    # Asign especific color and distance for vehicles in same lane, and default for the others\n",
    "    for i in range(len(pred_boxes)):\n",
    "        box = pred_boxes[i].cpu().numpy().astype(int)\n",
    "        label_id = pred_labels[i].item()\n",
    "        \n",
    "        # If the vehicle is in the same lane\n",
    "        if is_in_lane(lane_mask, box):\n",
    "            # Calculate distance\n",
    "            pixel_w = max(1, box[2] - box[0])\n",
    "            real_w = VEHICLE_CONFIG[label_id]\n",
    "            dist = (real_w * FOCAL_LENGTH) / pixel_w\n",
    "            \n",
    "            # Get the RGB color\n",
    "            color = get_distance_color(dist) \n",
    "            label = f\"Class: {label_id}\\nDistance: {dist:.1f}m\"\n",
    "        else:\n",
    "            # If it is in other lane\n",
    "            color = (0, 255, 255)           # Cyan\n",
    "            label = f\"Class: {label_id}\"\n",
    "\n",
    "        dynamic_colors.append(color)\n",
    "        dynamic_labels.append(label)\n",
    "\n",
    "    # Draw the predictions over the image\n",
    "    processed_image = draw_predictions(image_tensor, pred_masks, pred_boxes, dynamic_labels, \n",
    "                                       masks_colors=dynamic_colors, custom_labels=True)\n",
    "\n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f81cbe",
   "metadata": {},
   "source": [
    "Por último se va a implementar una función que dada una imagen, realiza todo el proceso mencionado anteriormente, para devolver la imagen con las distancias calculadas y representadas con los vehículos segmentados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad8548fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_distance(image, M, Minv, img_h, img_w, model, \n",
    "                     device='cpu', score_threshold=0.5, draw_lane=False):\n",
    "\n",
    "    filter_image, _ = road_line_filter_image(image)\n",
    "    no_persp = cv2.warpPerspective(filter_image, M, (img_w, img_h), flags=cv2.INTER_LINEAR)\n",
    "    left_fit, right_fit, _ = calculate_road_lines(no_persp)\n",
    "\n",
    "    # Create lane mask\n",
    "    lane_mask = np.zeros_like(image)\n",
    "\n",
    "    if left_fit is not None and right_fit is not None:\n",
    "        # Get the road mask\n",
    "        lane_mask = get_road_mask(no_persp, left_fit, right_fit, Minv, img_h, img_w)\n",
    "\n",
    "\n",
    "    # ------------ VEHICLE SEGMENTATION MASKS ------\n",
    "\n",
    "    # Convert the image to Tensor format\n",
    "    image_tensor = F_torch.to_tensor(image).to(device)\n",
    "    masks, boxes, labels = get_predictions(image_tensor, model, score_threshold)\n",
    "\n",
    "\n",
    "    # ------------ GENERATE RESULTING FRAME --------\n",
    "\n",
    "    # Draw the lane\n",
    "    if draw_lane:\n",
    "        image_processed = cv2.addWeighted(image, 1, lane_mask, 0.3, 0)\n",
    "        image_processed_tensor = F_torch.to_tensor(image_processed).to(device)\n",
    "\n",
    "    image_processed = draw_predictions_in_lanes(image_processed_tensor,\n",
    "                                                masks, boxes, labels, lane_mask)\n",
    "    \n",
    "    return image_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0c8a8",
   "metadata": {},
   "source": [
    "Se va a mostrar a continuación un ejemplo de este proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a2d0645",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision' has no attribute '_is_tracing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n\u001b[32m     24\u001b[39m Minv = np.linalg.inv(M)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m image_processed = \u001b[43mpredict_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMinv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdraw_lane\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m plt.figure(figsize=(\u001b[32m24\u001b[39m, \u001b[32m16\u001b[39m))\n\u001b[32m     29\u001b[39m plt.subplot(\u001b[32m2\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mpredict_distance\u001b[39m\u001b[34m(image, M, Minv, img_h, img_w, model, device, score_threshold, draw_lane)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# ------------ VEHICLE SEGMENTATION MASKS ------\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Convert the image to Tensor format\u001b[39;00m\n\u001b[32m     19\u001b[39m image_tensor = F_torch.to_tensor(image).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m masks, boxes, labels = \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ------------ GENERATE RESULTING FRAME --------\u001b[39;00m\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Draw the lane\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m draw_lane:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\predictions.py:90\u001b[39m, in \u001b[36mget_predictions\u001b[39m\u001b[34m(image_tensor, model, score_threshold)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Turn off gradient mode during the prediction execution\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# Get the predictions for that image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m prediction = predictions[\u001b[32m0\u001b[39m]\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Only keep predictions if the model has a certain level of confidence about them (over 50% score)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:117\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch.Tensor):\n\u001b[32m    116\u001b[39m     features = OrderedDict([(\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, features)])\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m proposals, proposal_losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m detections, detector_losses = \u001b[38;5;28mself\u001b[39m.roi_heads(features, proposals, images.image_sizes, targets)\n\u001b[32m    119\u001b[39m detections = \u001b[38;5;28mself\u001b[39m.transform.postprocess(\n\u001b[32m    120\u001b[39m     detections, images.image_sizes, original_image_sizes\n\u001b[32m    121\u001b[39m )  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:372\u001b[39m, in \u001b[36mRegionProposalNetwork.forward\u001b[39m\u001b[34m(self, images, features, targets)\u001b[39m\n\u001b[32m    370\u001b[39m proposals = \u001b[38;5;28mself\u001b[39m.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n\u001b[32m    371\u001b[39m proposals = proposals.view(num_images, -\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m boxes, scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilter_proposals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_anchors_per_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m losses = {}\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:277\u001b[39m, in \u001b[36mRegionProposalNetwork.filter_proposals\u001b[39m\u001b[34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[39m\n\u001b[32m    275\u001b[39m final_scores = []\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m boxes, scores, lvl, img_shape \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(proposals, objectness_prob, levels, image_shapes):\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     boxes = \u001b[43mbox_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_boxes_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# remove small boxes\u001b[39;00m\n\u001b[32m    280\u001b[39m     keep = box_ops.remove_small_boxes(boxes, \u001b[38;5;28mself\u001b[39m.min_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torchvision\\ops\\boxes.py:172\u001b[39m, in \u001b[36mclip_boxes_to_image\u001b[39m\u001b[34m(boxes, size)\u001b[39m\n\u001b[32m    169\u001b[39m boxes_y = boxes[..., \u001b[32m1\u001b[39m::\u001b[32m2\u001b[39m]\n\u001b[32m    170\u001b[39m height, width = size\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_is_tracing\u001b[49m():\n\u001b[32m    173\u001b[39m     boxes_x = torch.max(boxes_x, torch.tensor(\u001b[32m0\u001b[39m, dtype=boxes.dtype, device=boxes.device))\n\u001b[32m    174\u001b[39m     boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torchvision' has no attribute '_is_tracing'"
     ]
    }
   ],
   "source": [
    "# Get the image\n",
    "image_name = \"gta1.jpeg\"\n",
    "img_path = os.path.join(PATH_IMAGES, image_name)\n",
    "\n",
    "image = cv2.imread(img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img_h, img_w = image.shape[:2]\n",
    "\n",
    "p1_top_left = (int(img_w * 0.45), int(img_h * 0.63)) \n",
    "p2_top_right = (int(img_w * 0.52), int(img_h * 0.63)) \n",
    "p3_bot_right = (int(img_w * 0.84), int(img_h * 0.99)) \n",
    "p4_bot_left  = (int(img_w * 0.28), int(img_h * 0.99)) \n",
    "\n",
    "src_pts = np.float32([p1_top_left, p2_top_right, p3_bot_right, p4_bot_left])\n",
    "\n",
    "dst_pts = np.float32([(int(img_w*0.4), 0), \n",
    "                                  (int(img_w*0.6), 0), \n",
    "                                  (int(img_w*0.6), img_h), \n",
    "                                  (int(img_w*0.4), img_h)\n",
    "                    ])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "Minv = np.linalg.inv(M)\n",
    "\n",
    "image_processed = predict_distance(image, M, Minv, img_h, img_w, model, device, draw_lane=True)\n",
    "\n",
    "plt.figure(figsize=(24, 16))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(image_processed)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6cd9a",
   "metadata": {},
   "source": [
    "## Predicción en vídeo\n",
    "\n",
    "Se implementa a continuación una función que calcula las predicciones de distancia en una secuencia de vídeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_distance_video(video_path, output_path, model, device='cpu', score_threshold=0.5, \n",
    "                           frames_per_prediction=1, src_pts=None, dst_pts=None, focal_lenght=1000, draw_lane=False):\n",
    "    \"\"\"\n",
    "        This function procces a video by predicting the distance of the vechiles in the same lane that the vehicle from which the secuence is recorded.\n",
    "\n",
    "    Args:\n",
    "        video_path: The path to the video file\n",
    "        output_path: The path where the resulting video is written\n",
    "        model: The segmentation model that predicts masks and bouding boxes\n",
    "        device: The device where the computations will be done. CPU by default\n",
    "        score_threshold: The asurance of the model for shwoing the instance predictions \n",
    "        frames_per_prediction: The amount of frames that passes beetwen each prediction. The smaller, the faster the process, but results will be worst. Default is 1 (every frame)\n",
    "        src_pts: The points of a rectangle align with the road lines in the image from the perspective of the car.\n",
    "        dst_pts: The points of the src_pts rectanggle, but from a \"bird eye\" perspective.\n",
    "\n",
    "    Returns:\n",
    "        true if video is correctly processed, False if an error ocurred.\n",
    "    \"\"\"\n",
    "    # Control frames per second\n",
    "    if frames_per_prediction <= 0:\n",
    "        print(\"Frames per predictions must be > 0\\n\")\n",
    "        return False\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        return False\n",
    "    else:\n",
    "        # Create Writer with same properties that the original video\n",
    "        img_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        img_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS) \n",
    "        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (img_w, img_h))\n",
    "        \n",
    "        print(f\"Processing video... Resolution: {img_w}x{img_h}, FPS: {fps}\")\n",
    "\n",
    "        if src_pts is None:\n",
    "            p1_top_left = (int(img_w * 0.45), int(img_h * 0.63)) \n",
    "            p2_top_right = (int(img_w * 0.52), int(img_h * 0.63)) \n",
    "            p3_bot_right = (int(img_w * 0.84), int(img_h * 0.99)) \n",
    "            p4_bot_left  = (int(img_w * 0.28), int(img_h * 0.99)) \n",
    "\n",
    "            src_pts = np.float32([p1_top_left, p2_top_right, p3_bot_right, p4_bot_left])\n",
    "\n",
    "        if dst_pts is None:\n",
    "            dst_pts = np.float32([(int(img_w*0.4), 0), \n",
    "                                  (int(img_w*0.6), 0), \n",
    "                                  (int(img_w*0.6), img_h), \n",
    "                                  (int(img_w*0.4), img_h)\n",
    "                    ])\n",
    "            \n",
    "        # Calculate homography\n",
    "        M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "        Minv = np.linalg.inv(M)\n",
    "        \n",
    "        frame_count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break # Video ended\n",
    "\n",
    "            # ------------ LANE MASK ------------------------\n",
    "\n",
    "            filter_frame, _ = road_line_filter_image(frame)\n",
    "            no_persp = cv2.warpPerspective(filter_frame, M, (img_w, img_h), flags=cv2.INTER_LINEAR)\n",
    "            left_fit, right_fit, _ = calculate_road_lines(no_persp)\n",
    "\n",
    "            # Create lane mask\n",
    "            lane_mask = np.zeros_like(frame)\n",
    "\n",
    "            if left_fit is not None and right_fit is not None:\n",
    "                # Get the road mask\n",
    "                lane_mask = get_road_mask(no_persp, left_fit, right_fit, Minv, img_h, img_w)\n",
    "\n",
    "\n",
    "            # ------------ VEHICLE SEGMENTATION MASKS ------\n",
    "\n",
    "            # Convert the image to Tensor format\n",
    "            frame_tensor = F_torch.to_tensor(frame).to(device)\n",
    "            \n",
    "            # Let between model predictions as many frames as the parameter frames_per_prediction\n",
    "            if frame_count % frames_per_prediction == 0:\n",
    "                # Process the frame and write it in the ouput route\n",
    "                masks, boxes, labels = get_predictions(frame_tensor, model, score_threshold)\n",
    "\n",
    "\n",
    "            # ------------ GENERATE RESULTING FRAME --------\n",
    "\n",
    "            # Draw the lane\n",
    "            if draw_lane:\n",
    "                frame_processed = cv2.addWeighted(frame, 1, lane_mask, 0.3, 0)\n",
    "                frame_tensor = F_torch.to_tensor(frame_processed).to(device)\n",
    "\n",
    "            frame_processed = draw_predictions_in_lanes(frame_tensor,\n",
    "                                                        masks, \n",
    "                                                        boxes, \n",
    "                                                        labels, \n",
    "                                                        lane_mask)\n",
    "\n",
    "            out.write(frame_processed)\n",
    "            \n",
    "            # Show the frame processing in real time\n",
    "            cv2.imshow(\"Processed video\", frame_processed)\n",
    "            \n",
    "            frame_count += 1\n",
    "            if frame_count % 50 == 0:\n",
    "                print(f\"{frame_count} frames processed...\")\n",
    "\n",
    "            # Exit if press 'q'\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        if frame_count == 0:\n",
    "            print(\"Error, no frames have been processed\")\n",
    "            return False\n",
    "\n",
    "        # Liberar recursos\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa207910",
   "metadata": {},
   "source": [
    "Finalmente, se realiza la predicción de las distancias en un vídeo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video... Resolution: 1814x836, FPS: 30.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision' has no attribute '_is_tracing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m video_path = os.path.join(PATH_VIDEOS, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m output_path = os.path.join(PATH_RESULT_VIDEOS,\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_result.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[43mprocess_distance_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVideo saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mprocess_distance_video\u001b[39m\u001b[34m(video_path, output_path, model, device, score_threshold, frames_per_prediction, src_pts, dst_pts, focal_lenght, draw_lane)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Let between model predictions as many frames as the parameter frames_per_prediction\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame_count % frames_per_prediction == \u001b[32m0\u001b[39m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# Process the frame and write it in the ouput route\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     masks, boxes, labels = \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# ------------ GENERATE RESULTING FRAME --------\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Draw the lane\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m draw_lane:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\predictions.py:90\u001b[39m, in \u001b[36mget_predictions\u001b[39m\u001b[34m(image_tensor, model, score_threshold)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Turn off gradient mode during the prediction execution\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# Get the predictions for that image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m prediction = predictions[\u001b[32m0\u001b[39m]\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Only keep predictions if the model has a certain level of confidence about them (over 50% score)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:117\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch.Tensor):\n\u001b[32m    116\u001b[39m     features = OrderedDict([(\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, features)])\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m proposals, proposal_losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m detections, detector_losses = \u001b[38;5;28mself\u001b[39m.roi_heads(features, proposals, images.image_sizes, targets)\n\u001b[32m    119\u001b[39m detections = \u001b[38;5;28mself\u001b[39m.transform.postprocess(\n\u001b[32m    120\u001b[39m     detections, images.image_sizes, original_image_sizes\n\u001b[32m    121\u001b[39m )  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:372\u001b[39m, in \u001b[36mRegionProposalNetwork.forward\u001b[39m\u001b[34m(self, images, features, targets)\u001b[39m\n\u001b[32m    370\u001b[39m proposals = \u001b[38;5;28mself\u001b[39m.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n\u001b[32m    371\u001b[39m proposals = proposals.view(num_images, -\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m boxes, scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilter_proposals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_anchors_per_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m losses = {}\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\rpn.py:277\u001b[39m, in \u001b[36mRegionProposalNetwork.filter_proposals\u001b[39m\u001b[34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[39m\n\u001b[32m    275\u001b[39m final_scores = []\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m boxes, scores, lvl, img_shape \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(proposals, objectness_prob, levels, image_shapes):\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     boxes = \u001b[43mbox_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_boxes_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# remove small boxes\u001b[39;00m\n\u001b[32m    280\u001b[39m     keep = box_ops.remove_small_boxes(boxes, \u001b[38;5;28mself\u001b[39m.min_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Vision\\Trabajo_Opcional\\ADAS-car-distance-prediction\\car-distance-prediction\\.venv\\Lib\\site-packages\\torchvision\\ops\\boxes.py:172\u001b[39m, in \u001b[36mclip_boxes_to_image\u001b[39m\u001b[34m(boxes, size)\u001b[39m\n\u001b[32m    169\u001b[39m boxes_y = boxes[..., \u001b[32m1\u001b[39m::\u001b[32m2\u001b[39m]\n\u001b[32m    170\u001b[39m height, width = size\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_is_tracing\u001b[49m():\n\u001b[32m    173\u001b[39m     boxes_x = torch.max(boxes_x, torch.tensor(\u001b[32m0\u001b[39m, dtype=boxes.dtype, device=boxes.device))\n\u001b[32m    174\u001b[39m     boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torchvision' has no attribute '_is_tracing'"
     ]
    }
   ],
   "source": [
    "# Get the video capture\n",
    "video_name = \"video1\"\n",
    "video_path = os.path.join(PATH_VIDEOS, f\"{video_name}.mp4\")\n",
    "output_path = os.path.join(PATH_RESULT_VIDEOS,f\"{video_name}_result.mp4\")\n",
    "\n",
    "result = process_distance_video(video_path, output_path, model, device)\n",
    "\n",
    "if result:\n",
    "    print(f\"Video saved in: {output_path}\")\n",
    "else:\n",
    "    print(\"An error ocurred while openning the video.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
